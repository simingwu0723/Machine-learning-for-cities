{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLC HW 4\n",
    "\n",
    "### Question 1 (40 points)\n",
    "\n",
    "In this question, you will model traffic counts in Pittsburgh using Gaussian process (GP) regression.  The included dataset, \"PittsburghTrafficCounts.csv\", represents the average daily traffic counts computed by traffic sensors at over 1,100 locations in Allegheny County, PA.  The data was collected from years 2012-2014 and compiled by Carnegie Mellon University’s Traffic21 Institute; we have the longitude, latitude, and average daily count for each sensor.  \n",
    "\n",
    "Given this dataset, your goal is to learn a model of traffic count as a function of spatial location.  To do so, fit a Gaussian Process regression model to the observed data.  While you can decide on the precise kernel specification, you should try to achieve a good model fit, as quantified by a log marginal likelihood value greater than (i.e., less negative than) -1400.  Here are some hints for getting a good model fit:\n",
    "\n",
    "* We recommend that you take the logarithm of the traffic counts, and then subtract the mean of this vector, before fitting the model.\n",
    "\n",
    "* Since the data is noisy, don't forget to include a noise term (WhiteKernel) in your model.\n",
    "\n",
    "* When fitting a GP with RBF kernel on multidimensional data, you can learn a separate length scale for each dimension, e.g., length_scale=(length_scale_x, length_scale_y). \n",
    "\n",
    "Your Python code should provide the following five outputs:\n",
    "\n",
    "1) The kernel after parameter optimization and fitting to the observed data. (10 pts)\n",
    "\n",
    "2) The log marginal likelihood of the training data. (5 pts)\n",
    "\n",
    "3) Show a 2-D plot of the model's predictions over a mesh grid of longitude/latitude (with color corresponding to the model's predictions) and overlay a 2-D scatter plot of sensor locations (with color corresponding to the observed values). (10 pts)\n",
    "\n",
    "4) What percentage of sensors have average traffic counts more than two standard deviations higher or lower than the model predicts given their spatial location? (5 pts)\n",
    "\n",
    "5) Show a 2-D scatter plot of the sensor locations, with three colors corresponding to observed values a) more than two standard deviations higher than predicted, b) more then two standard deviations lower than predicted, and c) within two standard deviations of the predicted values. (10 pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "Data1=pd.read_csv(\"PittsburghTrafficCounts.csv\")\n",
    "Data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 2: Cluster-based anomaly detection (10 points)\n",
    "\n",
    "Given an unlabeled dataset with two real-valued attributes, we perform cluster-based anomaly detection by running k-means, choosing the number of clusters k automatically using the Schwarz criterion.  Four clusters are formed:\n",
    "\n",
    "A: 2 points, center (10, 10), standard deviation 1\n",
    "\n",
    "B: 200 points, center (15, 20), standard deviation 1\n",
    "\n",
    "C: 150 points, center (35, 5), standard deviation 5\n",
    "\n",
    "D: 100 points, center (0, 0), standard deviation 0.1\n",
    "\n",
    "Given the four points below, which of these points are, and are not, likely to be anomalies?  Choose “Anomaly” or “Not Anomaly”, and provide a brief explanation, for each point. \n",
    "(Hint: your answers should take into account the size and standard deviation of each cluster as well as the distances to cluster centers.)\n",
    "\n",
    "(10, 11)\tAnomaly / Not Anomaly\n",
    "\n",
    "(15, 19)\tAnomaly / Not Anomaly\n",
    "\n",
    "(35, 2)\t\tAnomaly / Not Anomaly\n",
    "\n",
    "(1, 0)\t\tAnomaly / Not Anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solutions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Anomaly detection (50 points)\n",
    "\n",
    "For this question, use the \"County Health Indicators\" dataset provided to identify the most anomalous counties. Please list the top 5 most anomalous counties computed using each of the following models.  (We recommend that, as a pre-processing step, you drop na values, and make sure all numeric values are treated as floats not strings.)\n",
    "\n",
    "Part 1: Learn a Bayesian network structure using only the six features [\"'\\% Smokers'\",\"'\\% Obese'\",\"'Violent Crime Rate'\",\"'80/20 Income Ratio'\",\"'\\% Children in Poverty'\",\"'Average Daily PM2.5'\"].  Use pd.cut() to discretize each feature into 5 categories: 0,1,2,3,4. \n",
    "\n",
    "(a) Use HillClimbSearch and BicScore to learn the Bayesian network structure (5 pts) \n",
    "\n",
    "(b) Which 5 counties have the lowest (most negative) log-likelihood values? Please show a ranked list of the top counties' names and log-likelihood values. (10 pts)\n",
    "\n",
    "Part 2: Cluster based anomaly detection.  Use all numeric features for this part, and do not discretize. \n",
    "\n",
    "(a) Clustering with k-means. Please use k=3 clusters. Compute each record's distance to the nearest cluster center and report the five counties which have the longest distances. (10 pts)\n",
    "\n",
    "(b) Cluster with Gaussian Mixture. Please repeat (2)a but use log-likelihood for each record (rather than distance) as the measure of anomalousness. (10 pts)\n",
    "\n",
    "Part 3: Choose one more anomaly detection model you prefer and report the top 5 most anomalous counties by the model you chose.  (10 pts)\n",
    "\n",
    "Part 4: Compare and contrast the results from the different models.  Were there some counties that were found to be anomalous in some models and not in others?  Please provide some intuitions on why each county was found to be anomalous. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data2=pd.read_csv(\"2022CountyHealthIndicators.csv\")\n",
    "Data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
