{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 6, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (5+5+5+5+5=25 pts)\n",
    "\n",
    "### Consider the dataset HW2_task1.csv, a binary classification problem with two real-valued input attributes.  As you can see from the plot, the dataset is linearly separable. Train a linear SVM (setting C=100000 just to emphasize that no slack variables are allowed) and  answer the following five questions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data1=pd.read_csv('HW2_task1.csv')\n",
    "X=data1.iloc[:,:2]\n",
    "Y=data1.iloc[:,2]\n",
    "plt.gca()\n",
    "plt.scatter(X.iloc[:,0], X.iloc[:,1], s=50, c=Y, cmap=plt.cm.get_cmap('coolwarm', 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Report the separating hyperplane (line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. List the support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Calculate the upper and lower hyperplanes (lines) of the margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Compute the width of the margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. What would you expect to happen to the margin if the constant C was made very small?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here.  You can justify your intuitions with code if you'd like, but this is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (4+8+8=20 pts)\n",
    "\n",
    "### Given the dataset provided below, answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "data2=pd.read_csv('HW2_task2.csv')\n",
    "X=data2.iloc[:,:2]\n",
    "Y=data2.iloc[:,2]\n",
    "\n",
    "plt.gca()\n",
    "plt.scatter(X.iloc[:,0], X.iloc[:,1], s=50, c=Y, cmap=plt.cm.get_cmap('coolwarm', 2));\n",
    "plt.show()\n",
    "\n",
    "# Generate training (X_train, Y_train) and testing (X_test, Y_test) datasets for out of sample test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Train a polynomial SVM using the training set. Use the default arguments, and report both in-sample (training set) and out-of-sample (test set) classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will use the validation subset in order to pick the optimal parameters for the polynomial model.\n",
    "\n",
    "### b. Try polynomials of degree 1,2,3,4. For each degree, consider a variety of regularization constants from the range C=[math.exp(i) for i in np.linspace(-10,2*degree,200)] in order to evaluate the classifier performance over the validation set defined below.  Plot the graph of \"Accuracy vs log(C)\" for each degree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we generate validation data (X_vali, Y_vali) from the training dataset. \n",
    "# Denote the remaining training data by (X_train_1, Y_train_1).\n",
    "X_train_1,X_vali,Y_train_1,Y_vali = train_test_split(X_train, Y_train, test_size=0.33, random_state=99)\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Choose the optimal degree and the optimal regularization constant C based on these graphs.  Use the optimal degree and C to compute and report the final out-of-sample accuracy of the best classification model selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (5+5=10pts).\n",
    "\n",
    "### This task is to be done by hand rather than using Python.\n",
    "\n",
    "Assume you have a data set as below. It contains records of cars with three features: the type of the car (sports or SUV), the color of the car (red or yellow), and the origin of the car (domestic or imported). And the labels for the data are yes (car was stolen) and no (car was not stolen).\n",
    "\n",
    "CarType,Color,Origin,Stolen?\n",
    "\n",
    "sports,red,domestic,yes\n",
    "\n",
    "SUV,red,imported,yes\n",
    "\n",
    "sports,yellow,domestic,no\n",
    "\n",
    "sports,red,imported,no\n",
    "\n",
    "SUV,red,domestic,yes\n",
    "\n",
    "SUV,red,imported,yes\n",
    "\n",
    "sports,yellow,domestic,no\n",
    "\n",
    "SUV,red,imported,yes\n",
    "\n",
    "SUV,yellow,domestic,yes\n",
    "\n",
    "sports,yellow,imported,no\n",
    "\n",
    "sports,red,imported,yes\n",
    "\n",
    "\n",
    "### Questions:\n",
    "\n",
    "a) Calculate the following sample probabilities:\n",
    "\n",
    "P(Yes)\n",
    "\n",
    "P(No)\n",
    "\n",
    "P(Red|Yes)\n",
    "\n",
    "P(SUV|Yes)\n",
    "\n",
    "P(Domestic|Yes)\n",
    "\n",
    "P(Red|No)\n",
    "\n",
    "P(SUV|No)\n",
    "\n",
    "P(Domestic|No)\n",
    "\n",
    "b) Using naive Bayes classification, what is the probability that a red, domestic SUV will be stolen? Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (4+2+2+2=10pts)\n",
    "\n",
    "Consider the following problem involving Gaussian Naive Bayes classification.  We use eight factors to predict if the monthly rent of an apartment in Brazil is lower than $600 or not. The variables are:\n",
    "\n",
    "y: The label (1 - rent is lower than 600 USD, 0 - rent is higher or equal to 600 USD)\n",
    "\n",
    "city: Apartment location (1 - city, 0 - otherwise)\n",
    "\n",
    "area: Code number for each area\n",
    "\n",
    "rooms: Number of rooms\n",
    "\n",
    "bathroom: Number of bathrooms\n",
    "\n",
    "parking_spaces: Number of parking spaces\n",
    "\n",
    "floor: Apartment floor\n",
    "\n",
    "animal: 1 - if animals are allowed, 0 - if animals are not allowed \n",
    "\n",
    "furniture: 1 - if apartment is furnished, 0 - if apartment is unfurnished \n",
    "\n",
    "Original dataset \"brazilian_houses_to_rent\" available in: https://www.kaggle.com/datasets/rubenssjr/brasilian-houses-to-rent.\n",
    "\n",
    "### Using the data provided below, learn a Naive Bayes classifier from the training data and answer the following questions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "data_train=pd.read_csv(\"HW2_task4_train.csv\") \n",
    "y_train=data_train.iloc[:,1] \n",
    "X_train=data_train.iloc[:,2:] \n",
    "\n",
    "# Testing data\n",
    "data_test=pd.read_csv(\"HW2_task4_test.csv\")\n",
    "y_test=data_test.iloc[:,1]\n",
    "X_test=data_test.iloc[:,2:]\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) What is the prediction accuracy for Naive Bayes, both in sample (on the training data) and out of sample (on the test data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) What is the prior probability of \"rent lower than 600 USD\", learned from the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) What is the mean and variance of of each input for apartments with rent lower than 600 USD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) What is the mean and variance of each input variable for apartments with rent higher than 600 USD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 (10+15=25 pts)\n",
    "\n",
    "We have an artificial data set split, where the training set contains both labeled and unlabeled data. Column 'y' is the label, and columns '0','1','2' are categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv(\"HW2_task5_train.csv\")\n",
    "y_Labeled_train=data_train.iloc[:,1] \n",
    "X_Labeled_train=data_train.iloc[:,2:] \n",
    "\n",
    "data_test=pd.read_csv(\"HW2_task5_test.csv\")\n",
    "y_Labeled_test=data_test.iloc[:,1]\n",
    "X_Labeled_test=data_test.iloc[:,2:]\n",
    "\n",
    "data_Unlabeled=pd.read_csv(\"HW2_task5_unlabeled.csv\")\n",
    "X_Unlabeled_train=data_Unlabeled.iloc[:,1:]\n",
    "\n",
    "print(X_Labeled_train)\n",
    "print(y_Labeled_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Learn a discrete Naive Bayes classifier from  X_Labeled_train, use it to predict the labels of X_Labeled_test, and report the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Improve the classification by using the unlabeled data, data_Unlabeled and the EM algorithm to predict labels of X_Labeled_test, and report the new accuracy by EM semi-supervised algorithm (Please feel free to use code from our lab sessions or any packages you prefer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
