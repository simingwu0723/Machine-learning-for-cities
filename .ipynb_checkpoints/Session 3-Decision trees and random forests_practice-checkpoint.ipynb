{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Scikit-learn (sklearn)\n",
    "\n",
    "1. Preprocessing (covered last week).\n",
    "\n",
    "2. Supervised Learning.\n",
    "\n",
    "3. Model selection.\n",
    "\n",
    "4. Decision trees.\n",
    "\n",
    "5. Random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your version and make sure >0.18\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn has a ton of methods implemented, many of which we will see later in the course!\n",
    "\n",
    "### Supervised Learning \n",
    "\n",
    "(Regression/Classification)\n",
    "\n",
    "Linear Models (Ordinary Least Squares, Logistic Regression, Lasso and Ridge...)\n",
    "\n",
    "Kernel regression\n",
    "\n",
    "SVM\n",
    "\n",
    "Gaussian Processes\n",
    "\n",
    "Decision Trees and Random Forests (next class)\n",
    "\n",
    "Naive Bayes\n",
    "\n",
    "Supervised Neural Network models (incl. Deep Learning)\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Clustering.\n",
    "\n",
    "Dimension Reduction.\n",
    "\n",
    "Representation in Neural Networks such as RBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Spectral)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an appropriate machine learning model.  Here we create an instance of logistic regression classifier.\n",
    "logreg = linear_model.LogisticRegression()\n",
    "\n",
    "# Train the model with the \"fit\" function, and predict using the \"predict\" function\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# now the color represents the predicted class\n",
    "plt.scatter(X[:, 0], X[:, 1], c=logreg.predict(X), cmap=plt.cm.Spectral)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips: How to use packages from sklearn.\n",
    "\n",
    "Step one: What is the problem we want to solve and what is the model we want to fit. \n",
    "\n",
    "Step two: What are the hyper-parameters related to model structure.\n",
    "\n",
    "Step three: What are the inputs dataframe and what are the parameters we want to tune.\n",
    "\n",
    "Step four: What are hyper-parameters for training process. (learning rate, iteration max...)\n",
    "\n",
    "step five: What are the outputs and tuned parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalized Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random((10,10))\n",
    "y = 5*(X[:,1]-0.5)+3*(X[:,5]-0.5)+np.random.random(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso and Ridge \n",
    "\n",
    "Ridge:\n",
    "\n",
    "$\\underset{w}{min\\,} {{|| X w - y||_2}^2 + \\alpha {||w||_2}^2}$\n",
    "\n",
    "Lasso:\n",
    "\n",
    "$\\underset{w}{min\\,} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data set: X, y \n",
    "\n",
    "#### model: \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge\n",
    "\n",
    "#### Hyper-parameters to set\n",
    "(1) model related: alpha, fit_intercept\n",
    "\n",
    "(2) training related: normalize, copy_X, max_iter, tol, solver, random_state (defaults are generally fine for these)\n",
    "\n",
    "#### Model parameters to learn (output)\n",
    "\n",
    "intercept_, coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Lasso model using the given X and y. Lasso has the nice feature that many irrelevant attributes have weights \n",
    "# reduced to 0 (i.e., it gives a sparse model)\n",
    "clf = linear_model.Lasso(fit_intercept=True,alpha=0.1)\n",
    "clf.fit(X,y)\n",
    "\n",
    "# let's look at the tuned parameter values.  As expected, we can see larger values on columns 1 and 5.\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict y given X\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the in-sample R^2 value\n",
    "1-((clf.predict(X)-y)**2).mean()/y.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or equivalently:\n",
    "clf.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will consider a range of alpha values and visualize the tuned weights (parameters) for each alpha\n",
    "n_alphas = 200\n",
    "alphas = np.logspace(-5, 1, n_alphas)\n",
    "\n",
    "coefs = []\n",
    "R_2=[]\n",
    "for a in alphas:\n",
    "    clf = linear_model.Lasso(fit_intercept=True,alpha=a)\n",
    "    clf.fit(X,y)\n",
    "    coefs.append(clf.coef_)\n",
    "    R_2.append(1-((clf.predict(X)-y)**2).mean()/y.var())\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse x-axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Lasso coefficients as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(alphas, R_2)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('R_2')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection and Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Cross-validation: evaluating estimator performance\n",
    "\n",
    "http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "#### (2) Or it could be used for tuning hyper-parameters.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/grid_search.html#grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating estimator performance.\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing supervised machine learning to hold out part of the available data as a test set (X_test, y_test), then to learn a model on the remaining (training) data and evaluate its accuracy on the test data.\n",
    "\n",
    "#### Tuning hyper-parameters\n",
    "If you have a model with important hyper-parameters (e.g., the amount of penalization for Lasso or Ridge regression), you can tune (find good values of) these hyperparameters by further splitting the training data into a training and validation set (still keeping the test data separate from these for a final, unbiased measure of performance).  To tune the hyper-parameters you can try a range of parameter values, learning from the (reduced) training set and evaluating performance on the validation set, and choose the hyper-parameters with best validation set performance.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data: predicting housing price using 311 calls for service\n",
    "path = 'https://serv.cusp.nyu.edu/~cq299/ADS2016/Data/Bayesian/'\n",
    "data4=pd.read_csv(path + \"example4.csv\", low_memory=False)\n",
    "list_311=list(data4.loc[:,\"Adopt A Basket\":\"X Ray Machine Equipment\"].columns)\n",
    "data5=data4[[\"sale_price\",\"gross_sq_feet\",\"mean\"]+list_311]\n",
    "data5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.asarray(data5.iloc[:,1:])\n",
    "y=np.asarray(data5.sale_price)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-sample R^2 value for linear regression using the whole training dataset\n",
    "lm=linear_model.LinearRegression()\n",
    "lm.fit(X,y)\n",
    "1-((lm.predict(X)-y)**2).mean()/y.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well do we do out of sample?  Let's split the data into 60% training, 40% test, and average performance over 10 random splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "OS=[]\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = i)    \n",
    "    lm=linear_model.LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    OS.append(1-((lm.predict(X_test)-y_test)**2).mean()/y_test.var())   # could use OS.append(lm.score(X_test,y_test)) instead\n",
    "print(np.mean(OS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does ridge regression do better out of sample?  Let's try with an arbitrary choice of penalization parameter alpha = 1.\n",
    "OS=[]\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = i)    \n",
    "    lm=linear_model.Ridge(alpha=1)\n",
    "    lm.fit(X_train,y_train)\n",
    "    OS.append(1-((lm.predict(X_test)-y_test)**2).mean()/y_test.var()) # or equivalently: OS.append(lm.score(X_test,y_test))\n",
    "print(np.mean(OS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "\n",
    "Exhaustive Grid Search. The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tune the hyper-parameters for ridge regression and calculate the OS R-squared using the new tuned alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid ={'alpha':np.logspace(-3, 0, 50)}\n",
    "\n",
    "OS=[]\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = i)\n",
    "    rid=linear_model.Ridge()\n",
    "    gr=GridSearchCV(rid,param_grid=param_grid,cv=5)\n",
    "    rs=gr.fit(X_train,y_train)\n",
    "    print(rs.best_estimator_)\n",
    "    OS.append(1-((rs.predict(X_test)-y_test)**2).mean()/y_test.var())\n",
    "print(np.mean(OS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decision Tree. \n",
    "\n",
    "Stop-and-Frisk Data set: https://en.wikipedia.org/wiki/Stop-and-frisk_in_New_York_City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"session_3_stop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# remove records with any missing values\n",
    "data=data.dropna()\n",
    "\n",
    "# Let's take \"found.weapon\" as the target variable. \n",
    "y=data.loc[:,\"found.weapon\"]\n",
    "\n",
    "# Get the feature space.  We are using only features from before the stop, getting rid of features from during/after the stop like \"arrested\".\n",
    "X=data.loc[:,\"suspect.race\":\"time.period\"]\n",
    "X=pd.get_dummies(X)\n",
    "\n",
    "# Split data into 70% train, 30% test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.3, random_state=999)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part one: IS and OS Accuracy of prediction. \n",
    "\n",
    "DecisionTreeClassifier:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# learn model\n",
    "dt=DecisionTreeClassifier()\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# in sample accuracy\n",
    "print('In sample accuracy:',dt.score(X_train,y_train))\n",
    "\n",
    "# out of sample accuracy\n",
    "print('Out of sample accuracy:',dt.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #1: Get the average OS accuracy over 10 train/test splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does that mean our model is super good? However... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would our accuracy be if we predicted 0 (no weapon found) for everyone?\n",
    "print(1.*len(y_test[y_test==0])/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we have a skewed class distribution (96% \"no weapon found\"), let's use area under the receiver operating characteristic curve (\"ROC AUC\") instead of accuracy.\n",
    "\n",
    "ROC, Area under the curve:\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics\n",
    "\n",
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "\n",
    "Youtube:\n",
    "https://www.youtube.com/watch?v=hnRBl9-BzjQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "AUC_OS=[]\n",
    "for i in range(10):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "    dt=DecisionTreeClassifier()\n",
    "    dt.fit(X_train,y_train)\n",
    "    # predict_proba predicts the probability of each class rather than just the most likely class\n",
    "    pred=dt.predict_proba(X_test)[:,1] # predicted probability of y = 1\n",
    "    AUC_OS.append(roc_auc_score(np.array(y_test),pred))\n",
    "print(\"OS AUC\",np.mean(AUC_OS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well would we expect to do just by chance?\n",
    "chance_OS=[]\n",
    "for i in range(10):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "    pred=np.random.random(len(X_test))\n",
    "    chance_OS.append(roc_auc_score(np.array(y_test.apply(int)),pred))\n",
    "print(np.mean(chance_OS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control the complexity of the Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just use a single train/test split for this part:\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.3,random_state=999)\n",
    "AUC_OS=[]\n",
    "for i in range(2,500,25):\n",
    "    dt=DecisionTreeClassifier(max_leaf_nodes=i)\n",
    "    dt.fit(X_train,y_train)\n",
    "    AUC_OS.append(roc_auc_score(np.array(y_test),dt.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(range(2,500,25),AUC_OS)\n",
    "plt.xlabel(\"Max leaf nodes\")\n",
    "plt.ylabel(\"OS_AUC\")\n",
    "plt.title(\"AUC vs Complexity (max leaf nodes)\")\n",
    "plt.xlim(2,500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an aside: predict_proba vs. predict\n",
    "tm=pd.concat((pd.DataFrame(dt.predict_proba(X_test)),pd.DataFrame(dt.predict(X_test))),axis=1)\n",
    "tm.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# This time we'll use max_depth to control the complexity of the tree, still using the same train/test split as above,\n",
    "# and optimize the parameter value using GridSearchCV.\n",
    "param_grid = {'max_depth':range(1,11)}\n",
    "dt=DecisionTreeClassifier()\n",
    "gr=GridSearchCV(dt,param_grid=param_grid,scoring='roc_auc')\n",
    "rs=gr.fit(X_train,y_train)\n",
    "print(rs.best_estimator_)\n",
    "print(roc_auc_score(np.array(y_test),rs.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Decision trees can be used for feature selection by calculating the Gini importance of each feature (higher = more important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=8)\n",
    "dt.fit(X_train, y_train)\n",
    "Feature_importance=pd.DataFrame([list(X_train.columns),list(dt.feature_importances_)]).T\n",
    "Feature_importance.columns=[\"variables\",\"importance\"]\n",
    "\n",
    "# list the top 5 most important features in order\n",
    "Feature_importance.sort_values(by=\"importance\",ascending=False).iloc[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate our new training and testing model using the top three features.\n",
    "X_train_simple=X_train.loc[:,[\"stopped.bc.object\",\"location.housing_transit\",\"precinct\"]]\n",
    "X_test_simple=X_test.loc[:,[\"stopped.bc.object\",\"location.housing_transit\",\"precinct\"]]\n",
    "\n",
    "# Now let's see the performance of this simple model.\n",
    "dt = DecisionTreeClassifier(max_depth=8)\n",
    "dt.fit(X_train_simple, y_train)\n",
    "print(\"The AUC score for this simple model with 3 features is\",roc_auc_score(y_test,dt.predict_proba(X_test_simple)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Tree we built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=2) # just to keep it simple for visualization\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# display the output using www.webgraphviz.com, or if you have GraphViz installed on\n",
    "# your computer, you can use that\n",
    "print(tree.export_graphviz(dt,out_file=None,\n",
    "                         feature_names=X_train.columns.values,  \n",
    "                         class_names=['no weapon found','weapon found'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True,impurity=False).replace(\"<br/>\",\", \").replace(\"&le;\",\"<=\").replace(\"=<\",\"=\\\"\").replace(\">,\",\"\\\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to install GraphViz on your own machine:\n",
    "\n",
    "conda install graphviz\n",
    "\n",
    "pip install pydot\n",
    "\n",
    "pip install pydotplus\n",
    "\n",
    "For people who experienced this error: \"GraphViz's executables not found\"\n",
    "\n",
    "http://stackoverflow.com/questions/18438997/why-is-pydot-unable-to-find-graphvizs-executables-in-windows-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will only work if GraphViz is installed on your machine\n",
    "from sklearn import tree\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "thestring = tree.export_graphviz(dt, out_file=None,  \n",
    "                         feature_names=X_train.columns.values, \n",
    "                         class_names=['no weapon found','weapon found'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True,impurity=False)\n",
    "graph = pydotplus.graph_from_dot_data(thestring)  \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same training data as above\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=30, n_jobs=-1,max_leaf_nodes=10)\n",
    "rf.fit(X_train, y_train)\n",
    "pred=rf.predict_proba(X_test)[:,1]\n",
    "print(roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #2. Let's fix max_leaf_nodes=10, build forests with between 1 and 50 trees, and plot the AUC as a function of number of trees (n_estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #3.  Use GridSearchCV to optimize the hyperparameter, n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
